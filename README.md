![AIcrowd-Logo](https://raw.githubusercontent.com/AIcrowd/AIcrowd/master/app/assets/images/misc/aicrowd-horizontal.png)

# Robot open-Ended Autonomous Learning Starter Kit

Instructions to make submissions to the [Robot open-Ended Autonomous Learning 2020](https://www.aicrowd.com/challenges/real-robots-2020) competition.

Participants will have to submit their controllers, with packaging specifications, and the evaluator will automatically build a docker image and execute their controllers in two phases: an Intrinsic Phase and an Extrinsic Phase.
The Intrinsic Phase will only be run for the Final Evaluation.
During Round 1 and Round 2 only the Extrinsic Phase will be run on the evaluator: participants will run the Intrinsic Phase and make their controller learn locally.

### Dependencies 

- **Anaconda** (By following instructions [here](https://www.anaconda.com/download)) At least version `4.5.11` is required to correctly populate `environment.yml`.
- **real-robots** [PyPI](https://pypi.org/project/real-robots/)

### Setup

* Clone the repository 

```
git clone git@github.com:AIcrowd/REAL2020_starter_kit.git
cd REAL2020_starter_kit.git
```

* Create a conda environment from the provided `environment.yml`

```sh
conda env create -f environment.yml
```

* Activate the conda environment and install your code specific dependencies

```sh
conda activate real_robots
# If say you want to install PyTorch
# conda install pytorch torchvision -c pytorch
#
# or you can even use pip to install any additional packages
# for example : 
# pip install -U real-robots
# which updates the real-robots package to the latest version
```

### Test Submission Locally

* Test locally by running:
```
python local_evaluation.py
```
You can edit local_evaluation.py to run either the intrinsic or the extrinsic phase (or both) and adjust their duration and other parameters (i.e. number of objects) for testing purposes.
Regardless of the modification, when the solution will be evaluated, the intrinsic and extrinsic phases will be run for the duration and number of trials defined in the Rules. During Round 1 and Round 2 it is expected that the controller has already learned during a 15M timestep Intrinsic Phase run locally by the participants.

* (optional) build docker image locally and run docker container

```
pip install -U aicrowd-repo2docker
# This also expects that you have Docker, Nvidia-Docker installed

./debug.sh
```

# How do I specify my software runtime ?

The software runtime is specified by exporting your `conda` env to the root
of your repository by doing :

```
# The included environment.yml is generated by the command below, and you do not need to run it again
# if you did not add any custom dependencies

conda env export --no-build > environment.yml

# Note the `--no-build` flag, which is important if you want your anaconda env to be replicable across all
```

This `environment.yml` file will be used to recreate the `conda environment` inside the Docker container.
This repository includes an example `environment.yml`

You can specify your software environment by using all the [available configuration options of repo2docker](https://repo2docker.readthedocs.io/en/latest/config_files.html). (But please remember to use [aicrowd-repo2docker](https://pypi.org/project/aicrowd-repo2docker/) to have GPU support)

# How can I connect my system to competition?
You can use local_evaluation.py that in base the input will evaluate your system.

## Evaluate input
- controller: agent class instance with the follow methods: start_intrinsic_phase, start_estrinsic_phase e step  
- round: "R1" or "R2", which represent Round1 and Round2 of the competition respectevily 
- actions type: "cartesian", "joints" or "macro_action"
- total objects: n element of {1,2,3}
- intrinsic phase steps: interger number 
- estrinsic phase steps for each goal: interger number 
- total goal: integer number
- render: True if you want visualize the robot simulation
- goal file path: string

### Input description
Actions type:
- 'macro_action': Numpy.ndarray([(x1,y1),(x2,y2)]), where (x1,y1) is the start point and (x2,y2) is the end point of the trajectory.
- 'cartesian': Numpy.ndarray([x,y,z,o1,o2,o3,o4]), which it is the physics three dimensional space and the other points represent the gripper orientation.
- 'joints': Numpy.ndarray([x1,x2,x3,x4,x5,x6,x7,x8,x9]), where it represent the desidered position for each joint

Controller class instance:
It have to be a class with several methods which is called repetitively. These is explained in [policy.py](https://github.com/AIcrowd/real_robots/blob/master/real_robots/policy.py). Here we specified only step method:
- step:
    - input: observation, reward, done
      - observation is a dictionary with several keys. Most are descripted in [environment.md](https://github.com/AIcrowd/real_robots/blob/master/environment.md) and the rest below:
        - retina: it is a rgb image (dimension: 240x320x3) with view from above of the table that show also the robot arm.
        - object_positions: it is a dictionary with a key for each object on the table with associated the seven-dimensional position.
        - mask: it is a filtered image (dimension: 240x320) with view from above of the table that show also the robot arm.
        - goal: it is the rgb image that represent the goal.
        - goal_positions: it is the objects position desidered.
        - goal_mask: it is the filtered image that represent the goal.
    - output: action
      - action is a dictionary with two keys:
        - action type: action
        - render: boolean\
        The evaluate class pass the dictionary (example: {'macro_action': Numpy.ndarray([ [0.1 , 0.3] , [0.2 , 0.3] ]), 'render': True}) to environment that will execute the specified action and it showing the simulation



# How do I can use simplifications?
## Actions space reduction:
  - 'macro_action': it allows to reduce from joints space to four-dimensional space, where the four points (x1,y1,x2,y2) represent a trajectory on the table that starts from (x1,y1) and ends to (x2,y2).
  - 'cartesian': it allows to reduce from joints space to seven-dimensional space, where the seven points (x,y,z,o1,o2,o3,o4) represent the three-dimensional point in the space and the gripper orientation desidered. 
  
## Abstraction simplifications contained in the observations:
  - coordinates: it allows to reduce from images space to seven-dimensional space, where the seven points (x,y,z,o1,o2,o3,o4) represent the physics three dimensional space and the other points represent the object orientation.
  - masks: it allows to reduce from images space to filtered images space, where a mask is an image that for each pixel has a integer number that let you know which object is in that pixel. (example: a cell with -1 represent the background pixel)

# What should my code structure be like ?

Please follow the structure documented in the included [my_controller.py](https://github.com/AIcrowd/REAL2020_starter_kit/blob/master/my_controller.py) to adapt
your already existing code to the required structure for this round.

## Important Concepts

### Repository Structure

- `aicrowd.json`
  Each repository should have a `aicrowd.json` with the following content :

```json
{
  "challenge_id": "real-robots-2020",
  "grader_id": "real-robots-2020",
  "authors": ["mohanty"],
  "description": "Robot open-Ended Autonomous Learning 2020 (REAL2020) Challenge.",
  "license" : "MIT",
  "debug": true
}
```


This is used to map your submission to the said challenge, so please remember to use the correct `challenge_id` and `grader_id` as specified above.

If you set `debug` to `true`, then the evaluation will run on a separate set of 20 environments, and the logs from your submitted code (if it fails), will be made available to you to help you debug.
**NOTE** : **IMPORTANT** : By default we have set `debug:false`, so when you have done the basic integration testing of your code, and are ready to make a final submission, please do make sure to set `debug` to `true` in `aicrowd.json`.

- `my_controller.py`
  The task is to implement your own controller my following the template provided in `my_controller.py`.
  The `my_controller.py` file should finally reference the implemented class as `SubmittedPolicy`

## Submission

To make a submission, you will have to create a private repository on [https://gitlab.aicrowd.com/](https://gitlab.aicrowd.com/).

You will have to add your SSH Keys to your GitLab account by following the instructions [here](https://docs.gitlab.com/ee/gitlab-basics/create-your-ssh-keys.html).
If you do not have SSH Keys, you will first need to [generate one](https://docs.gitlab.com/ee/ssh/README.html#generating-a-new-ssh-key-pair).

Then you can create a submission by making a _tag push_ to your repository on [https://gitlab.aicrowd.com/](https://gitlab.aicrowd.com/).
**Any tag push (where the tag name begins with "submission-") to your private repository is considered as a submission**  
Then you can add the correct git remote, and finally submit by doing :

```
cd neurips_goal_real_robots_starter_kit
# Add AIcrowd git remote endpoint
git remote add aicrowd git@gitlab.aicrowd.com:<YOUR_AICROWD_USER_NAME>/REAL2020_starter_kit.git
git push aicrowd master

# Create a tag for your submission and push
git tag -am "submission-v0.1" submission-v0.1
git push aicrowd master
git push aicrowd submission-v0.1

# Note : If the contents of your repository (latest commit hash) does not change,
# then pushing a new tag will **not** trigger a new evaluation.
```

You now should be able to see the details of your submission at :
[https://gitlab.aicrowd.com/<YOUR_AICROWD_USER_NAME>/REAL2020_starter_kit/issues](https://gitlab.aicrowd.com/<YOUR_AICROWD_USER_NAME>/neurips_goal_real_robots_starter_kit/issues)

**NOTE**: Remember to update your username in the link above :wink:

In the link above, you should start seeing something like this take shape (the whole evaluation can take a bit of time, so please be a bit patient too :wink: ) :

![image](https://user-images.githubusercontent.com/18488075/63256469-ed524780-c277-11e9-825a-4ea5acbbcf6c.png)


**Best of Luck** :tada: :tada:


# Author
**[Sharada Mohanty](https://twitter.com/MeMohanty)**  
**Emilio Cartoni**\
**Davide Montella**
